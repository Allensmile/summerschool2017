{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates a simple convolutional neural network, a variation of the LeNet5 model.\n",
    "\n",
    "It is written using Lasagne for simplicity, which should make it easier to play with the structure of the model, its regularization, update rule, etc.  A pure Theano implementation is available [here](http://deeplearning.net/tutorial/lenet.html#lenet) if you are curious.\n",
    "\n",
    "The training loop is written in regular Python and implements early stopping on a validation set.\n",
    "\n",
    "This example should take a total of around 9 minutes to train on a GRID K520 with cuDNN v5.1.\n",
    "\n",
    "Possible changes you could try:\n",
    "- change the nonlinearity of the convolution to rectifier unit\n",
    "- add an extra mlp layer\n",
    "- add dropout\n",
    "- change the update rule to Adam\n",
    "- limit the number of epoch of training to allow iterating more rapidly on code change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "from lasagne.layers import InputLayer, Conv2DLayer, MaxPool2DLayer, NonlinearityLayer, DenseLayer\n",
    "\n",
    "from load_data import load_data\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(23455)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To enable the GPU, run the following code\n",
    "import theano.gpuarray\n",
    "theano.config.floatX = 'float32'\n",
    "# theano.gpuarray.use('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation simplifies the model in the following ways:\n",
    "\n",
    " - LeNetConvPool doesn't implement location-specific gain and bias parameters\n",
    " - LeNetConvPool doesn't implement pooling by average, it implements pooling\n",
    "   by max.\n",
    " - Digit classification is implemented with a logistic regression rather than\n",
    "   an RBF network\n",
    " - LeNet5 was not fully-connected convolutions at second layer\n",
    "\n",
    "References:\n",
    " - Y. LeCun, L. Bottou, Y. Bengio and P. Haffner:\n",
    "   Gradient-Based Learning Applied to Document\n",
    "   Recognition, Proceedings of the IEEE, 86(11):2278-2324, November 1998.\n",
    "   http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define symbolic inputs\n",
    "x = T.matrix('x')\n",
    "y = T.ivector('y')\n",
    "\n",
    "nonlinearity = lasagne.nonlinearities.tanh\n",
    "\n",
    "## Build the architecture of the network\n",
    "# Input\n",
    "input_var = x.reshape((-1, 1, 28, 28))\n",
    "layer0 = lasagne.layers.InputLayer(shape=(None, 1, 28, 28), input_var=input_var)\n",
    "\n",
    "# First conv / pool / nonlinearity block\n",
    "conv1 = Conv2DLayer(layer0, num_filters=20, filter_size=(5, 5), nonlinearity=None)\n",
    "pool1 = MaxPool2DLayer(conv1, pool_size=(2, 2))\n",
    "act1 = NonlinearityLayer(pool1, nonlinearity=nonlinearity)\n",
    "\n",
    "# Second conv / pool / nonlinearity block\n",
    "conv2 = Conv2DLayer(act1, num_filters=50, filter_size=(5, 5), nonlinearity=None)\n",
    "pool2 = MaxPool2DLayer(conv2, pool_size=(2, 2))\n",
    "act2 = NonlinearityLayer(pool2, nonlinearity=nonlinearity)\n",
    "\n",
    "# Fully-connected layer\n",
    "dense1 = DenseLayer(act2, num_units=500, nonlinearity=nonlinearity)\n",
    "\n",
    "# Fully-connected layer for the output\n",
    "network = DenseLayer(dense1, num_units=10, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "## Training\n",
    "# Prediction and cost\n",
    "prediction = lasagne.layers.get_output(network)\n",
    "loss = lasagne.objectives.categorical_crossentropy(prediction, y)\n",
    "loss = loss.mean()\n",
    "\n",
    "# Gradients and updates\n",
    "params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "updates = lasagne.updates.sgd(loss, params, learning_rate=0.1)\n",
    "train_fn = theano.function([x, y], loss, updates=updates)\n",
    "\n",
    "## Monitoring and evaluation\n",
    "test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "test_loss = lasagne.objectives.categorical_crossentropy(test_prediction, y)\n",
    "test_loss = test_loss.mean()\n",
    "\n",
    "# Misclassification rate\n",
    "test_err = T.mean(T.neq(T.argmax(test_prediction, axis=1), y),\n",
    "                  dtype=theano.config.floatX)\n",
    "\n",
    "valid_fn = theano.function([x, y], test_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate_model(train_fn, valid_fn, datasets, n_epochs, batch_size):\n",
    "    train_set_x, train_set_y = datasets[0]\n",
    "    valid_set_x, valid_set_y = datasets[1]\n",
    "    test_set_x, test_set_y = datasets[2]\n",
    "\n",
    "    # compute number of minibatches for training, validation and testing\n",
    "    n_train_batches = train_set_x.shape[0] // batch_size\n",
    "    n_valid_batches = valid_set_x.shape[0] // batch_size\n",
    "    n_test_batches = test_set_x.shape[0] // batch_size\n",
    "\n",
    "    ## early-stopping parameters\n",
    "    # look as this many examples regardless\n",
    "    patience = 10000\n",
    "    # wait this much longer when a new best is found\n",
    "    patience_increase = 2\n",
    "    # a relative improvement of this much is considered significant\n",
    "    improvement_threshold = 0.995\n",
    "    # Go through this many minibatches before checking the network\n",
    "    # on the validation set; in this case we check every epoch\n",
    "    validation_frequency = min(n_train_batches, patience // 2)\n",
    "\n",
    "    best_validation_loss = np.inf\n",
    "    best_iter = 0\n",
    "    test_score = 0.\n",
    "    start_time = time.clock()\n",
    "\n",
    "    epoch = 0\n",
    "    done_looping = False\n",
    "\n",
    "    while (epoch < n_epochs) and (not done_looping):\n",
    "        epoch = epoch + 1\n",
    "        for minibatch_index in range(n_train_batches):\n",
    "\n",
    "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "\n",
    "            if iter % 100 == 0:\n",
    "                print('training @ iter = %i' % iter)\n",
    "            cost_ij = train_fn(train_set_x[minibatch_index * batch_size:(minibatch_index + 1) * batch_size],\n",
    "                               train_set_y[minibatch_index * batch_size:(minibatch_index + 1) * batch_size])\n",
    "\n",
    "            if (iter + 1) % validation_frequency == 0:\n",
    "\n",
    "                # compute zero-one loss on validation set\n",
    "                validation_losses = [valid_fn(valid_set_x[i * batch_size:(i + 1) * batch_size],\n",
    "                                              valid_set_y[i * batch_size:(i + 1) * batch_size])\n",
    "                                     for i in range(n_valid_batches)]\n",
    "                this_validation_loss = np.mean(validation_losses)\n",
    "                print('epoch %i, minibatch %i/%i, validation error %f %%' %\n",
    "                      (epoch, minibatch_index + 1, n_train_batches,\n",
    "                       this_validation_loss * 100.))\n",
    "\n",
    "                # if we got the best validation score until now\n",
    "                if this_validation_loss < best_validation_loss:\n",
    "\n",
    "                    #improve patience if loss improvement is good enough\n",
    "                    if this_validation_loss < best_validation_loss *  \\\n",
    "                       improvement_threshold:\n",
    "                        patience = max(patience, iter * patience_increase)\n",
    "\n",
    "                    # save best validation score and iteration number\n",
    "                    best_validation_loss = this_validation_loss\n",
    "                    best_iter = iter\n",
    "\n",
    "                    # test it on the test set\n",
    "                    test_losses = [\n",
    "                        valid_fn(test_set_x[i * batch_size:(i + 1) * batch_size],\n",
    "                                 test_set_y[i * batch_size:(i + 1) * batch_size])\n",
    "                        for i in range(n_test_batches)]\n",
    "                    test_score = np.mean(test_losses)\n",
    "                    print(('     epoch %i, minibatch %i/%i, test error of '\n",
    "                           'best model %f %%') %\n",
    "                          (epoch, minibatch_index + 1, n_train_batches,\n",
    "                           test_score * 100.))\n",
    "\n",
    "            if patience <= iter:\n",
    "                done_looping = True\n",
    "                break\n",
    "\n",
    "    end_time = time.clock()\n",
    "    print('Optimization complete.')\n",
    "    print('Best validation score of %f %% obtained at iteration %i, '\n",
    "          'with test performance %f %%' %\n",
    "          (best_validation_loss * 100., best_iter + 1, test_score * 100.))\n",
    "    print('The code ran for %.2fm' % ((end_time - start_time) / 60.))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading data\n"
     ]
    }
   ],
   "source": [
    "datasets = load_data('mnist.pkl.gz', shared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training @ iter = 0\n",
      "epoch 1, minibatch 100/100, validation error 7.520000 %\n",
      "     epoch 1, minibatch 100/100, test error of best model 7.739999 %\n",
      "training @ iter = 100\n",
      "epoch 2, minibatch 100/100, validation error 4.830000 %\n",
      "     epoch 2, minibatch 100/100, test error of best model 5.020000 %\n",
      "training @ iter = 200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-2760e16dc775>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-991e92bc931a>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(train_fn, valid_fn, datasets, n_epochs, batch_size)\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training @ iter = %i'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             cost_ij = train_fn(train_set_x[minibatch_index * batch_size:(minibatch_index + 1) * batch_size],\n\u001b[0;32m---> 39\u001b[0;31m                                train_set_y[minibatch_index * batch_size:(minibatch_index + 1) * batch_size])\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mvalidation_frequency\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/blip/code/Theano/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "evaluate_model(train_fn, valid_fn, datasets, n_epochs=200, batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
